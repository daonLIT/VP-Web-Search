from __future__ import annotations

import json
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime, timezone
from hashlib import sha256
import random
from pathlib import Path

from bs4 import BeautifulSoup
import requests
from urllib.parse import urljoin, urlparse
import re
from typing import List, Dict, Any, Optional
import time

from urllib.parse import parse_qs, urlencode, urlparse, urlunparse

from langchain_core.tools import tool
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI

from langchain_tavily import TavilySearch, TavilyExtract


def _hash_text(text: str) -> str:
    return sha256(text.encode("utf-8", errors="ignore")).hexdigest()


def _normalize_tavily_search_output(output: Any) -> List[Dict[str, Any]]:
    """
    TavilySearch.invoke ê²°ê³¼ëŠ” ë³´í†µ dict {'results': [...]} í˜•íƒœ. (ë²„ì „ì— ë”°ë¼ listì¼ ìˆ˜ë„ ìžˆìŒ)
    - listë©´ ê·¸ëŒ€ë¡œ
    - dictë©´ output['results'] ì‚¬ìš©
    """
    if output is None:
        return []
    if isinstance(output, list):
        return output
    if isinstance(output, dict):
        res = output.get("results")
        return res if isinstance(res, list) else []
    return []


def build_tools(vectordb: Chroma) -> List[Any]:
    # -----------------------------
    # 1) Vector search (ìžˆì§€ë§Œ web-only ê·¸ëž˜í”„ì—ì„œëŠ” ì•ˆ ì”€)
    # -----------------------------
    @tool("search_existing_guidance")
    def search_existing_guidance(
        phishing_type: str,
        scenario_hint: str = "",
        top_k: int = 3,
    ) -> Dict[str, Any]:
        """
        DBì— ì €ìž¥ëœ ë³´ì´ìŠ¤í”¼ì‹± ë¦¬í¬íŠ¸/ìŠ¤ë‹ˆíŽ«ì—ì„œ íŠ¹ì • ìœ í˜•ì˜ ì§€ì¹¨ì„ ê²€ìƒ‰í•œë‹¤.
        
        ìž…ë ¥:
        - phishing_type: ë³´ì´ìŠ¤í”¼ì‹± ìœ í˜• (ì˜ˆ: "ê²€ê²½ ì‚¬ì¹­", "ê¸°ê´€ ì‚¬ì¹­", "ê°€ì¡± ì‚¬ì¹­")
        - scenario_hint: ì‹œë‚˜ë¦¬ì˜¤ ížŒíŠ¸ (ì˜ˆ: "ê²€ì°° ì‚¬ì¹­í•´ì„œ í˜„ê¸ˆ íŽ¸ì·¨")
        - top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        ì¶œë ¥:
        {
            "found": bool,
            "count": int,
            "guidances": [
                {
                    "type": str,
                    "keywords": [str],
                    "scenario": [str],
                    "red_flags": [str],
                    "recommended_actions": [str],
                    "source_id": str,
                    "relevance_score": float
                }
            ]
        }
        """
        col = vectordb._collection
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_query = f"{phishing_type} {scenario_hint}".strip()
        
        # 1) ë¦¬í¬íŠ¸ kindì—ì„œ ê²€ìƒ‰
        report_where = {"kind": {"$eq": "voicephishing_report_v1"}}
        report_data = col.get(where=report_where, limit=50, include=["documents", "metadatas"])
        
        # 2) ìœ ì‚¬ë„ ê²€ìƒ‰ (ë²¡í„° ê²€ìƒ‰)
        vector_results = vectordb.similarity_search_with_relevance_scores(
            search_query, 
            k=top_k * 2,
            filter={"kind": "voicephishing_report_v1"}
        )
        
        guidances = []
        seen_ids = set()
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ìš°ì„  ì²˜ë¦¬
        for doc, score in vector_results:
            content = doc.page_content
            meta = doc.metadata
            
            # ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸ íŒŒì‹± (ìœ í˜•ë³„ ì„¹ì…˜ ì¶”ì¶œ)
            try:
                # ë¦¬í¬íŠ¸ê°€ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¼ê³  ê°€ì •
                type_match = _extract_type_from_report(content, phishing_type)
                if type_match and type_match["type"] not in seen_ids:
                    seen_ids.add(type_match["type"])
                    type_match["source_id"] = meta.get("report_id", "")
                    type_match["relevance_score"] = float(score)
                    guidances.append(type_match)
                    
                    if len(guidances) >= top_k:
                        break
            except Exception:
                continue
        
        return {
            "found": len(guidances) > 0,
            "count": len(guidances),
            "guidances": guidances
        }


    def _extract_type_from_report(report_text: str, target_type: str) -> Optional[Dict[str, Any]]:
        """
        ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ìœ í˜•ì˜ ì •ë³´ë¥¼ ì¶”ì¶œí•œë‹¤.
        """
        import re
        
        # ìœ í˜• ì„¹ì…˜ ì°¾ê¸° (ì˜ˆ: "ìœ í˜•: ê²€ê²½ ì‚¬ì¹­")
        type_pattern = rf"ìœ í˜•:\s*([^\n]+)"
        type_matches = list(re.finditer(type_pattern, report_text))
        
        for match in type_matches:
            found_type = match.group(1).strip()
            
            # ìœ í˜•ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸ (ë¶€ë¶„ ì¼ì¹˜ í—ˆìš©)
            if target_type.lower() in found_type.lower() or found_type.lower() in target_type.lower():
                # í•´ë‹¹ ìœ í˜• ì„¹ì…˜ ì¶”ì¶œ
                section_start = match.start()
                
                # ë‹¤ìŒ "ìœ í˜•:" ë˜ëŠ” ë¬¸ì„œ ëê¹Œì§€
                next_match = None
                for m in type_matches:
                    if m.start() > section_start:
                        next_match = m
                        break
                
                section_end = next_match.start() if next_match else len(report_text)
                section = report_text[section_start:section_end]
                
                # ì„¹ì…˜ì—ì„œ ì •ë³´ ì¶”ì¶œ
                keywords = _extract_field(section, r"ì£¼ìš” í‚¤ì›Œë“œ:\s*([^\n]+)")
                scenario = _extract_scenario(section)
                red_flags = _extract_list_field(section, r"ì˜ì‹¬ ì‹ í˜¸|ê·¼ê±° snippet_id")
                
                return {
                    "type": found_type,
                    "keywords": keywords,
                    "scenario": scenario,
                    "red_flags": red_flags,
                    "recommended_actions": []  # ë¦¬í¬íŠ¸ì— ë”°ë¼ ì¶”ê°€
                }
        
        return None


    def _extract_field(text: str, pattern: str) -> List[str]:
        """ì •ê·œì‹ìœ¼ë¡œ í•„ë“œ ì¶”ì¶œ í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        import re
        match = re.search(pattern, text)
        if match:
            content = match.group(1).strip()
            # ì‰¼í‘œë‚˜ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„
            return [k.strip() for k in re.split(r'[,ï¼Œã€]', content) if k.strip()]
        return []


    def _extract_scenario(text: str) -> List[str]:
        """ì‹œë‚˜ë¦¬ì˜¤ ë‹¨ê³„ ì¶”ì¶œ"""
        import re
        scenario_pattern = r"ì‹œë‚˜ë¦¬ì˜¤:\s*((?:\d+[\.\)]\s*[^\n]+\n?)+)"
        match = re.search(scenario_pattern, text)
        if match:
            steps = match.group(1).strip().split('\n')
            return [re.sub(r'^\d+[\.\)]\s*', '', s).strip() for s in steps if s.strip()]
        return []


    def _extract_list_field(text: str, header_pattern: str) -> List[str]:
        """ë¦¬ìŠ¤íŠ¸ í˜•íƒœ í•„ë“œ ì¶”ì¶œ"""
        import re
        pattern = rf"{header_pattern}:?\s*((?:[-\*â€¢]\s*[^\n]+\n?)+)"
        match = re.search(pattern, text)
        if match:
            items = match.group(1).strip().split('\n')
            return [re.sub(r'^[-\*â€¢]\s*', '', item).strip() for item in items if item.strip()]
        return []


    @tool("generate_targeted_guidance")
    def generate_targeted_guidance(
        phishing_type: str,
        scenario: str,
        victim_profile: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        íŠ¹ì • ìœ í˜•ê³¼ ì‹œë‚˜ë¦¬ì˜¤ì— ë§žì¶˜ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ ,
        í”¼í•´ìž í”„ë¡œí•„ì„ ê³ ë ¤í•œ ë§žì¶¤í˜• ì§€ì¹¨ì„ ìƒì„±í•œë‹¤.
        
        ìž…ë ¥:
        - phishing_type: ë³´ì´ìŠ¤í”¼ì‹± ìœ í˜•
        - scenario: ì‹œë‚˜ë¦¬ì˜¤ ì„¤ëª…
        - victim_profile: í”¼í•´ìž íŠ¹ì„± (ì„ íƒ)
        
        ì¶œë ¥:
        {
            "type": str,
            "keywords": [str],
            "scenario": [str],
            "red_flags": [str],
            "recommended_actions": [str],
            "sources": [{title, url}]
        }
        """
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, timeout=30)
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        base_queries = [
            f"ë³´ì´ìŠ¤í”¼ì‹± {phishing_type} ìˆ˜ë²•",
            f"{phishing_type} {scenario}",
            f"{phishing_type} ì‹œë‚˜ë¦¬ì˜¤",
        ]
        
        # í”¼í•´ìž í”„ë¡œí•„ ê¸°ë°˜ ì¶”ê°€ í‚¤ì›Œë“œ
        if victim_profile:
            age = victim_profile.get("age")
            occupation = victim_profile.get("occupation")
            if age:
                base_queries.append(f"{phishing_type} {age}ëŒ€ í”¼í•´")
            if occupation:
                base_queries.append(f"{phishing_type} {occupation} ëŒ€ìƒ")
        
        # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
        all_snippets = []
        all_sources = []
        
        for query in base_queries[:3]:  # ìµœëŒ€ 3ê°œ ì¿¼ë¦¬
            args = {
                "query": query,
                "topic": "news",
                "max_results": 3,
                "time_range": "month",
            }
            
            raw_out = tavily_snippets.invoke(args)
            results = _normalize_tavily_search_output(raw_out)
            
            for r in results:
                url = (r.get("url") or "").strip()
                if url and not _is_hub_url(url):
                    all_snippets.append({
                        "title": r.get("title", ""),
                        "url": url,
                        "content": (r.get("content") or "")[:600],
                    })
                    all_sources.append({"title": r.get("title", ""), "url": url})
        
        # LLMìœ¼ë¡œ ì§€ì¹¨ ìƒì„±
        snippet_text = "\n\n".join([
            f"ì¶œì²˜: {s['title']}\nURL: {s['url']}\në‚´ìš©: {s['content']}"
            for s in all_snippets[:8]
        ])
        
        victim_context = ""
        if victim_profile:
            victim_context = f"\ní”¼í•´ìž íŠ¹ì„±: {json.dumps(victim_profile, ensure_ascii=False)}"
        
        prompt = f"""
    ë„ˆëŠ” ë³´ì´ìŠ¤í”¼ì‹± ìˆ˜ë²• ë¶„ì„ ì „ë¬¸ê°€ë‹¤.
    ì•„ëž˜ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ '{phishing_type}' ìœ í˜•ì˜ ìƒì„¸ ì§€ì¹¨ì„ ìƒì„±í•˜ë¼.

    ì‹œë‚˜ë¦¬ì˜¤ ížŒíŠ¸: {scenario}{victim_context}

    ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ JSON):
    {{
    "type": "{phishing_type}",
    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...],
    "scenario": [
        "1ë‹¨ê³„: ...",
        "2ë‹¨ê³„: ...",
        "3ë‹¨ê³„: ...",
        ...
    ],
    "red_flags": ["ì˜ì‹¬ ì‹ í˜¸1", "ì˜ì‹¬ ì‹ í˜¸2", ...],
    "recommended_actions": ["ëŒ€ì‘ë²•1", "ëŒ€ì‘ë²•2", ...]
    }}

    ê·œì¹™:
    - scenarioëŠ” 5~7ë‹¨ê³„ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ìž‘ì„±
    - ê²€ìƒ‰ ê²°ê³¼ì— ê·¼ê±°í•œ ë‚´ìš©ë§Œ í¬í•¨
    - í”¼í•´ìž íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§žì¶¤í˜• ë‚´ìš© ìž‘ì„±

    [ê²€ìƒ‰ ê²°ê³¼]
    {snippet_text}
    """.strip()
        
        response = llm.invoke(prompt).content.strip()
        
        # JSON íŒŒì‹±
        try:
            # ì½”ë“œ ë¸”ë¡ ì œê±°
            if response.startswith("```"):
                response = response.split("```")[1]
                if response.startswith("json"):
                    response = response[4:]
            
            guidance = json.loads(response)
            guidance["sources"] = all_sources[:5]
            
            return guidance
        except Exception as e:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
            return {
                "type": phishing_type,
                "keywords": [phishing_type, scenario],
                "scenario": [scenario],
                "red_flags": [],
                "recommended_actions": [],
                "sources": all_sources[:5],
                "error": str(e)
            }


    @tool("store_guidance_to_db")
    def store_guidance_to_db(
        guidance: Dict[str, Any],
        source_system: str = "external_request",
    ) -> Dict[str, Any]:
        """
        ìƒì„±ëœ ì§€ì¹¨ì„ DBì— ì €ìž¥í•œë‹¤.
        
        ìž…ë ¥:
        - guidance: generate_targeted_guidance ì¶œë ¥
        - source_system: ìš”ì²­ ì¶œì²˜
        
        ì¶œë ¥:
        {"stored": 1, "guidance_id": "..."}
        """
        now = datetime.now(timezone.utc).isoformat()
        
        # JSON ë¬¸ìžì—´ë¡œ ì €ìž¥
        content = json.dumps(guidance, ensure_ascii=False)
        guidance_id = _hash_text(content)
        
        doc = Document(
            page_content=content,
            metadata={
                "kind": "voicephishing_guidance_v1",
                "phishing_type": guidance.get("type", ""),
                "source_system": source_system,
                "created_at": now,
                "guidance_id": guidance_id,
            }
        )
        
        vectordb.add_documents([doc])
        
        return {"stored": 1, "guidance_id": guidance_id}
    
    @tool("crawl_site_for_phishing_cases")
    def crawl_site_for_phishing_cases(
        site_url: str,
        keywords: List[str] = None,
        max_articles: int = 10,
        article_selector: Optional[str] = None,
        title_selector: Optional[str] = None,
        link_selector: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        íŠ¹ì • ì‚¬ì´íŠ¸ì˜ ëª©ë¡ íŽ˜ì´ì§€ì—ì„œ ë³´ì´ìŠ¤í”¼ì‹± ê´€ë ¨ ê¸€ì„ í•„í„°ë§í•˜ê³  ë§í¬ë¥¼ ìˆ˜ì§‘í•œë‹¤.
        
        ìž…ë ¥:
        - site_url: í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ URL (ì˜ˆ: ê²½ì°°ì²­ ê³µì§€ì‚¬í•­ ëª©ë¡ íŽ˜ì´ì§€)
        - keywords: í•„í„°ë§ í‚¤ì›Œë“œ (ê¸°ë³¸: ["ë³´ì´ìŠ¤í”¼ì‹±", "ì „í™”ê¸ˆìœµì‚¬ê¸°", "ìŠ¤ë¯¸ì‹±", "í”¼ì‹±"])
        - max_articles: ìµœëŒ€ ìˆ˜ì§‘ ê¸€ ìˆ˜
        - article_selector: ê¸€ ëª©ë¡ CSS ì…€ë ‰í„° (ì„ íƒ, ìžë™ ê°ì§€ ì‹œë„)
        - title_selector: ì œëª© CSS ì…€ë ‰í„° (ì„ íƒ)
        - link_selector: ë§í¬ CSS ì…€ë ‰í„° (ì„ íƒ)
        
        ì¶œë ¥:
        {
            "site_url": str,
            "found_count": int,
            "articles": [
                {"title": str, "url": str, "matched_keywords": [str]},
                ...
            ]
        }
        """
        if keywords is None:
            keywords = [
                "ë³´ì´ìŠ¤í”¼ì‹±", "ì „í™”ê¸ˆìœµì‚¬ê¸°", "ìŠ¤ë¯¸ì‹±", "í”¼ì‹±", 
                "ë©”ì‹ ì €í”¼ì‹±", "ì‚¬ê¸°", "ê¸ˆìœµì‚¬ê¸°", "í…”ë ˆê·¸ëž¨"
            ]
        
        try:
            # User-Agent ì„¤ì • (ì°¨ë‹¨ ë°©ì§€)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(site_url, headers=headers, timeout=15)
            response.raise_for_status()
            response.encoding = response.apparent_encoding or 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ìžë™ ì…€ë ‰í„° ê°ì§€ ë˜ëŠ” ì§€ì •ëœ ì…€ë ‰í„° ì‚¬ìš©
            articles = []
            
            if article_selector:
                # ì‚¬ìš©ìž ì§€ì • ì…€ë ‰í„°
                items = soup.select(article_selector)
            else:
                # ìžë™ ê°ì§€: ì¼ë°˜ì ì¸ ê²Œì‹œíŒ íŒ¨í„´ë“¤
                items = (
                    soup.select('tr') or  # í…Œì´ë¸” ê¸°ë°˜
                    soup.select('li') or  # ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜
                    soup.select('article') or
                    soup.select('.board-list tr') or
                    soup.select('.notice-list li')
                )
            
            filtered_articles = []
            
            for item in items[:100]:  # ìµœëŒ€ 100ê°œê¹Œì§€ë§Œ íƒìƒ‰
                # ì œëª© ì¶”ì¶œ
                if title_selector:
                    title_elem = item.select_one(title_selector)
                else:
                    # ìžë™ ê°ì§€
                    title_elem = (
                        item.select_one('td.title') or
                        item.select_one('.title') or
                        item.select_one('a') or
                        item.select_one('h3') or
                        item.select_one('h4')
                    )
                
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                
                # í‚¤ì›Œë“œ í•„í„°ë§
                matched_keywords = [kw for kw in keywords if kw in title]
                if not matched_keywords:
                    continue
                
                # ë§í¬ ì¶”ì¶œ
                if link_selector:
                    link_elem = item.select_one(link_selector)
                else:
                    # ìžë™ ê°ì§€
                    link_elem = title_elem if title_elem.name == 'a' else item.select_one('a')
                
                if not link_elem:
                    continue
                
                href = link_elem.get('href', '')
                if not href:
                    continue
                
                # ìƒëŒ€ URL â†’ ì ˆëŒ€ URL ë³€í™˜
                full_url = urljoin(site_url, href)
                
                filtered_articles.append({
                    "title": title,
                    "url": full_url,
                    "matched_keywords": matched_keywords
                })
                
                if len(filtered_articles) >= max_articles:
                    break
            
            return {
                "site_url": site_url,
                "found_count": len(filtered_articles),
                "articles": filtered_articles
            }
        
        except Exception as e:
            return {
                "site_url": site_url,
                "found_count": 0,
                "articles": [],
                "error": str(e)
            }


    @tool("extract_article_content")
    def extract_article_content(
        article_url: str,
        content_selector: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        ê°œë³„ ê¸€ì˜ ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•œë‹¤.
        
        ìž…ë ¥:
        - article_url: ê¸€ ìƒì„¸ íŽ˜ì´ì§€ URL
        - content_selector: ë³¸ë¬¸ CSS ì…€ë ‰í„° (ì„ íƒ, ìžë™ ê°ì§€ ì‹œë„)
        
        ì¶œë ¥:
        {
            "url": str,
            "title": str,
            "content": str,
            "extracted": bool
        }
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(article_url, headers=headers, timeout=15)
            response.raise_for_status()
            response.encoding = response.apparent_encoding or 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title_elem = (
                soup.select_one('h1') or
                soup.select_one('h2.title') or
                soup.select_one('.subject') or
                soup.select_one('.post-title')
            )
            title = title_elem.get_text(strip=True) if title_elem else ""
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            if content_selector:
                content_elem = soup.select_one(content_selector)
            else:
                # ìžë™ ê°ì§€: ì¼ë°˜ì ì¸ ë³¸ë¬¸ íŒ¨í„´ë“¤
                content_elem = (
                    soup.select_one('div.content') or
                    soup.select_one('div.post-content') or
                    soup.select_one('div.article-body') or
                    soup.select_one('div#content') or
                    soup.select_one('article') or
                    soup.select_one('.view-content') or
                    soup.select_one('.board-view')
                )
            
            if not content_elem:
                # fallback: bodyì—ì„œ script/style ì œê±° í›„ ì¶”ì¶œ
                for tag in soup(['script', 'style', 'nav', 'header', 'footer']):
                    tag.decompose()
                content_elem = soup.select_one('body')
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
            content = content_elem.get_text(separator='\n', strip=True) if content_elem else ""
            
            # ê³¼ë„í•œ ê³µë°± ì œê±°
            content = re.sub(r'\n\s*\n', '\n\n', content)
            content = re.sub(r' +', ' ', content)
            
            return {
                "url": article_url,
                "title": title,
                "content": content[:5000],  # ìµœëŒ€ 5000ìž
                "extracted": bool(content)
            }
        
        except Exception as e:
            return {
                "url": article_url,
                "title": "",
                "content": "",
                "extracted": False,
                "error": str(e)
            }


    @tool("crawl_and_extract_batch")
    def crawl_and_extract_batch(
        site_url: str,
        keywords: List[str] = None,
        max_articles: int = 10,
        delay_seconds: float = 1.0,
    ) -> Dict[str, Any]:
        """
        ì‚¬ì´íŠ¸ í¬ë¡¤ë§ + ë³¸ë¬¸ ì¶”ì¶œì„ í•œë²ˆì— ì²˜ë¦¬í•œë‹¤.
        
        ìž…ë ¥:
        - site_url: ëª©ë¡ íŽ˜ì´ì§€ URL
        - keywords: í•„í„°ë§ í‚¤ì›Œë“œ
        - max_articles: ìµœëŒ€ ìˆ˜ì§‘ ê¸€ ìˆ˜
        - delay_seconds: ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        
        ì¶œë ¥:
        {
            "site_url": str,
            "crawled_count": int,
            "extracted_count": int,
            "articles": [
                {
                    "title": str,
                    "url": str,
                    "content": str,
                    "matched_keywords": [str]
                },
                ...
            ]
        }
        """
        # 1ë‹¨ê³„: ëª©ë¡ì—ì„œ ê´€ë ¨ ê¸€ ìˆ˜ì§‘
        crawl_result = crawl_site_for_phishing_cases.invoke({
            "site_url": site_url,
            "keywords": keywords,
            "max_articles": max_articles
        })
        
        if crawl_result.get("found_count", 0) == 0:
            return {
                "site_url": site_url,
                "crawled_count": 0,
                "extracted_count": 0,
                "articles": [],
                "note": "no_articles_found"
            }
        
        # 2ë‹¨ê³„: ê° ê¸€ì˜ ë³¸ë¬¸ ì¶”ì¶œ
        articles_with_content = []
        
        for article in crawl_result.get("articles", []):
            # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
            time.sleep(delay_seconds)
            
            extract_result = extract_article_content.invoke({"article_url": article["url"]})
            
            if extract_result.get("extracted"):
                articles_with_content.append({
                    "title": article["title"],
                    "url": article["url"],
                    "content": extract_result["content"],
                    "matched_keywords": article.get("matched_keywords", [])
                })
        
        return {
            "site_url": site_url,
            "crawled_count": crawl_result.get("found_count", 0),
            "extracted_count": len(articles_with_content),
            "articles": articles_with_content
        }


    @tool("generate_guidance_from_crawled_articles")
    def generate_guidance_from_crawled_articles(
        articles: List[Dict[str, Any]],
        target_type: Optional[str] = None,
        force_generate: bool = True,
    ) -> Dict[str, Any]:
        """
        í¬ë¡¤ë§í•œ ê¸€ë“¤ë¡œë¶€í„° ë³´ì´ìŠ¤í”¼ì‹± ì§€ì¹¨ì„ ìƒì„±í•œë‹¤.
        force_generate=Trueë©´ ë¬´ì¡°ê±´ ìµœì†Œ 1ê°œ ìœ í˜• ìƒì„±
        
        ìž…ë ¥:
        - articles: crawl_and_extract_batchì˜ articles ê²°ê³¼
        - target_type: íŠ¹ì • ìœ í˜•ìœ¼ë¡œ í•œì • (ì„ íƒ)
        
        ì¶œë ¥:
        {
            "guidance": {...},
            "source_articles": [{title, url}, ...]
        }
        """
        if not articles:
            return {
                "guidance": {"types": []},
                "source_articles": [],
                "note": "no_articles_provided"
            }

        
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, timeout=40)
        
        # ê¸€ ë‚´ìš© ìš”ì•½
        article_summaries = []
        for i, article in enumerate(articles[:15], 1):
            title = article.get("title", "")
            content = article.get("content", "")[:1500]  # ê° ê¸€ë‹¹ 1500ìž ì œí•œ
            
            article_summaries.append(
                f"{i}. ì œëª©: {title}\në‚´ìš©: {content}\n"
            )
        
        articles_text = "\n---\n".join(article_summaries)
        
        type_instruction = f"íŠ¹ížˆ '{target_type}' ìœ í˜•ì— ì§‘ì¤‘í•˜ë¼." if target_type else ""
        
        prompt = f"""
    ë„ˆëŠ” ë³´ì´ìŠ¤í”¼ì‹± ìˆ˜ë²• ë¶„ì„ ì „ë¬¸ê°€ë‹¤.
    ì•„ëž˜ëŠ” ê³µì‹ ê¸°ê´€ì—ì„œ í¬ë¡¤ë§í•œ ë³´ì´ìŠ¤í”¼ì‹± ê´€ë ¨ ê¸€ë“¤ì´ë‹¤.

    ì¤‘ìš” ì§€ì¹¨:
    1. ê¸€ì´ ì§ì ‘ì ì¸ ì‚¬ë¡€ê°€ ì•„ë‹ˆë”ë¼ë„, ì–¸ê¸‰ëœ ìˆ˜ë²•/íŒ¨í„´ì„ ì¶”ì¶œí•˜ë¼
    2. "ì˜ˆë°©", "ì£¼ì˜", "ì¡°ì‹¬" ë“±ì˜ ë§¥ë½ì—ì„œ ë‚˜ì˜¨ ìˆ˜ë²• ì„¤ëª…ë„ í¬í•¨
    3. ìµœì†Œ 1ê°œ ì´ìƒì˜ ìœ í˜•ì€ ë°˜ë“œì‹œ ì¶”ì¶œí•˜ë¼
    4. êµ¬ì²´ì  ì‚¬ë¡€ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ íŒ¨í„´ì´ë¼ë„ ì •ë¦¬í•˜ë¼

    {type_instruction}

    ì¶œë ¥ í˜•ì‹ (JSONë§Œ, ì½”ë“œë¸”ë¡ ì—†ì´):
    {{
    "types": [
        {{
        "type": "ìœ í˜•ëª… (ì˜ˆ: ê¸°ê´€ ì‚¬ì¹­, ê°€ì¡± ì‚¬ì¹­, ëŒ€ì¶œ ì‚¬ê¸°, AI ìŒì„± ì‚¬ì¹­ ë“±)",
        "keywords": ["í•µì‹¬í‚¤ì›Œë“œ1", "í•µì‹¬í‚¤ì›Œë“œ2", ...],
        "scenario": [
            "1ë‹¨ê³„: ì´ˆê¸° ì ‘ê·¼ (ì–´ë–»ê²Œ ì—°ë½í•˜ëŠ”ê°€)",
            "2ë‹¨ê³„: ì‹ ë¢° êµ¬ì¶• (ì–´ë–»ê²Œ ë¯¿ê²Œ ë§Œë“œëŠ”ê°€)",
            "3ë‹¨ê³„: ì •ë³´ íšë“ (ë¬´ì—‡ì„ ìš”êµ¬í•˜ëŠ”ê°€)",
            "4ë‹¨ê³„: ì••ë°• ì „ìˆ  (ì–´ë–»ê²Œ ê¸‰ë°•í•˜ê²Œ ë§Œë“œëŠ”ê°€)",
            "5ë‹¨ê³„: ê¸ˆì „ ìš”êµ¬ (ëˆì„ ì–´ë–»ê²Œ ë¹¼ê°€ëŠ”ê°€)"
        ],
        "red_flags": [
            "ì˜ì‹¬í•  ìˆ˜ ìžˆëŠ” ì‹ í˜¸ 1",
            "ì˜ì‹¬í•  ìˆ˜ ìžˆëŠ” ì‹ í˜¸ 2",
            ...
        ],
        "recommended_actions": [
            "ì¦‰ì‹œ ì·¨í•  í–‰ë™ 1",
            "ì¦‰ì‹œ ì·¨í•  í–‰ë™ 2",
            ...
        ],
        "real_cases": [
            "ê¸€ì—ì„œ ì–¸ê¸‰ëœ ì‚¬ë¡€ë‚˜ íŒ¨í„´ ìš”ì•½ 1",
            "ê¸€ì—ì„œ ì–¸ê¸‰ëœ ì‚¬ë¡€ë‚˜ íŒ¨í„´ ìš”ì•½ 2"
        ]
        }}
    ]
    }}

    ê·œì¹™:
    1. typesëŠ” ìµœì†Œ 1ê°œ, ìµœëŒ€ 5ê°œ
    2. scenarioëŠ” ì •í™•ížˆ 5ë‹¨ê³„ (ë¶€ì¡±í•˜ë©´ ì¼ë°˜ì  íŒ¨í„´ìœ¼ë¡œ ì±„ì›Œë¼)
    3. real_casesê°€ ì—†ìœ¼ë©´ ê¸€ì—ì„œ ì–¸ê¸‰ëœ ì˜ˆë°©ë²•/ì£¼ì˜ì‚¬í•­ì´ë¼ë„ ìš”ì•½
    4. ê¸€ì´ ë³´ë„ìžë£Œ/ê³µì§€ë¼ë„ ê·¸ ì•ˆì—ì„œ ìˆ˜ë²• ì •ë³´ ì¶”ì¶œ

    ì˜ˆì‹œ í•´ì„:
    - "AI ìŒì„±ìœ¼ë¡œ ë³´ì´ìŠ¤í”¼ì‹± ì˜ˆë°©" â†’ AI ìŒì„± ì‚¬ì¹­ ìˆ˜ë²•ì´ ìžˆë‹¤ëŠ” ì˜ë¯¸
    - "ê°€ì¡± ì‚¬ì¹­ ì£¼ì˜" â†’ ê°€ì¡± ì‚¬ì¹­ ìœ í˜• ì¶”ì¶œ
    - "ê³„ì¢Œ ì´ì²´ ìš”êµ¬ ì¡°ì‹¬" â†’ ê¸ˆì „ ìš”êµ¬ ë‹¨ê³„ì— í¬í•¨

    [í¬ë¡¤ë§í•œ ê¸€ë“¤]
    {articles_text}
    """.strip()
        
        # LLM í˜¸ì¶œ ì „ ë¡œê¹…
        print(f"\nðŸ“Š LLMì— ì „ë‹¬í•  ë‚´ìš©:")
        print(f"   - ê¸€ ê°œìˆ˜: {len(articles)}")
        print(f"   - ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(articles_text)} ìž")
        print(f"   - ì²« ë²ˆì§¸ ê¸€ ë¯¸ë¦¬ë³´ê¸°: {articles[0].get('title', '')[:50]}...")
        
        try:
            response = llm.invoke(prompt).content.strip()

            # ì‘ë‹µ ë¡œê¹…
            print(f"\nðŸ¤– LLM ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°:")
            print(response[:300] + "...")
            
            # JSON ì¶”ì¶œ
            if "```json" in response:
                response = response.split("```json")[1].split("```")[0]
            elif "```" in response:
                response = response.split("```")[1].split("```")[0]
            
            guidance_data = json.loads(response)

            # types ê²€ì¦
            types_list = guidance_data.get("types", [])

            # typesê°€ ë¹„ì–´ìžˆìœ¼ë©´ ê¸°ë³¸ í…œí”Œë¦¿
            if force_generate and len(types_list) == 0:
                print("âš ï¸  LLMì´ ìœ í˜•ì„ ìƒì„±í•˜ì§€ ì•ŠìŒ â†’ ê°•ì œ ìƒì„±")
                
                # ê¸€ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                all_titles = " ".join([a.get("title", "") for a in articles])
                extracted_keywords = []
                
                keyword_patterns = [
                    "ë³´ì´ìŠ¤í”¼ì‹±", "ìŠ¤ë¯¸ì‹±", "í”¼ì‹±", "ë©”ì‹ ì €", "AI", "ìŒì„±",
                    "ê°€ì¡±", "ê²€ì°°", "ê²½ì°°", "ê¸ˆìœµ", "ì€í–‰", "ëŒ€ì¶œ", "íˆ¬ìž"
                ]
                
                for kw in keyword_patterns:
                    if kw in all_titles:
                        extracted_keywords.append(kw)
                # ìµœì†Œí•œ ì–¸ê¸‰ëœ í‚¤ì›Œë“œë¡œ 1ê°œ ìœ í˜• ìƒì„±
                fallback_type = {
                    "type": target_type or "ë³´ì´ìŠ¤í”¼ì‹± ì¼ë°˜",
                    "keywords": extracted_keywords[:5] or ["ë³´ì´ìŠ¤í”¼ì‹±"],
                    "scenario": [
                        "1ë‹¨ê³„: ê³µê³µê¸°ê´€/ê¸ˆìœµê¸°ê´€ ì‚¬ì¹­ ì „í™”",
                        "2ë‹¨ê³„: ë²”ì£„ ì—°ë£¨/ê³„ì¢Œ ë¬¸ì œ ë“± ìœ„ê¸° ì¡°ì„±",
                        "3ë‹¨ê³„: ê°œì¸ì •ë³´ ìš”êµ¬",
                        "4ë‹¨ê³„: ì¦‰ì‹œ ì¡°ì¹˜ ì••ë°•",
                        "5ë‹¨ê³„: ê³„ì¢Œ ì´ì²´ ë˜ëŠ” ì•± ì„¤ì¹˜ ìœ ë„"
                    ],
                    "red_flags": [
                        "ì¶œì²˜ ë¶ˆëª… ì „í™”/ë¬¸ìž",
                        "ê¸´ê¸‰ ìƒí™© ê°•ì¡°",
                        "ê°œì¸ì •ë³´/ê¸ˆìœµì •ë³´ ìš”êµ¬"
                    ],
                    "recommended_actions": [
                        "í†µí™” ì¦‰ì‹œ ì¢…ë£Œ",
                        "ê²½ì°°ì²­ 182 ì‹ ê³ ",
                        "ê³µì‹ ê¸°ê´€ ë²ˆí˜¸ë¡œ ìž¬í™•ì¸"
                    ],
                    "real_cases": [
                        f"í¬ë¡¤ë§í•œ {len(articles)}ê°œ ê¸€ì—ì„œ ì–¸ê¸‰ëœ ì˜ˆë°©ë²• ê¸°ë°˜"
                    ]
                }
                
                guidance_data["types"] = [fallback_type]
            
            # ì¶œì²˜ ì •ë³´ ì¶”ê°€
            source_articles = [
                {"title": a.get("title", ""), "url": a.get("url", "")}
                for a in articles[:10]
            ]
            
            return {
                "guidance": guidance_data,
                "source_articles": source_articles
            }
        
        except Exception as e:
            print(f"âš ï¸  LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        
            fallback_guidance = {
                "types": [{
                    "type": "ë³´ì´ìŠ¤í”¼ì‹± ì¼ë°˜",
                    "keywords": ["ë³´ì´ìŠ¤í”¼ì‹±", "ì „í™”ê¸ˆìœµì‚¬ê¸°"],
                    "scenario": [
                        "1ë‹¨ê³„: ê³µê³µê¸°ê´€ ì‚¬ì¹­",
                        "2ë‹¨ê³„: ìœ„ê¸° ìƒí™© ì¡°ì„±",
                        "3ë‹¨ê³„: ê°œì¸ì •ë³´ ìš”êµ¬",
                        "4ë‹¨ê³„: ì¦‰ì‹œ ì¡°ì¹˜ ì••ë°•",
                        "5ë‹¨ê³„: ê¸ˆì „ ìš”êµ¬"
                    ],
                    "red_flags": ["ì¶œì²˜ ë¶ˆëª… ì—°ë½", "ê¸´ê¸‰ ìƒí™© ê°•ì¡°"],
                    "recommended_actions": ["í†µí™” ì¢…ë£Œ", "ê²½ì°° ì‹ ê³ "],
                    "real_cases": [f"{len(articles)}ê°œ ê¸€ ê¸°ë°˜"]
                }]
            }
            
            return {
                "guidance": fallback_guidance,
                "source_articles": [
                    {"title": a.get("title", ""), "url": a.get("url", "")}
                    for a in articles[:5]
                ],
                "error": str(e)
            }

    @tool("store_crawled_guidance")
    def store_crawled_guidance(
        guidance_data: Dict[str, Any],
        site_url: str,
        source_articles: List[Dict[str, str]],
    ) -> Dict[str, Any]:
        """
        í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„±í•œ ì§€ì¹¨ì„ DBì— ì €ìž¥í•œë‹¤.
        
        ìž…ë ¥:
        - guidance_data: generate_guidance_from_crawled_articlesì˜ guidance
        - site_url: í¬ë¡¤ë§í•œ ì‚¬ì´íŠ¸ URL
        - source_articles: ì¶œì²˜ ê¸€ ëª©ë¡
        
        ì¶œë ¥:
        {"stored": int, "guidance_ids": [str]}
        """
        now = datetime.now(timezone.utc).isoformat()
        stored_ids = []
        
        types_list = guidance_data.get("types", [])
        
        for type_info in types_list:
            content = json.dumps(type_info, ensure_ascii=False)
            guidance_id = _hash_text(content + now)
            
            doc = Document(
                page_content=content,
                metadata={
                    "kind": "voicephishing_guidance_crawled_v1",
                    "phishing_type": type_info.get("type", ""),
                    "source_site": site_url,
                    "source_articles_json": json.dumps(source_articles, ensure_ascii=False),
                    "created_at": now,
                    "guidance_id": guidance_id,
                }
            )
            
            vectordb.add_documents([doc])
            stored_ids.append(guidance_id)
        
        return {
            "stored": len(stored_ids),
            "guidance_ids": stored_ids
        }
    
    @tool("crawl_site_with_pagination")
    def crawl_site_with_pagination(
        site_url: str,
        keywords: List[str] = None,
        max_articles: int = 30,
        max_pages: int = 5,
        pagination_type: str = "auto",  # auto | url_param | path | next_button
        page_param: str = "page",  # URL íŒŒë¼ë¯¸í„° ì´ë¦„
        delay_seconds: float = 2.0,
    ) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ íŽ˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ë³´ì´ìŠ¤í”¼ì‹± ê´€ë ¨ ê¸€ì„ ìˆ˜ì§‘í•œë‹¤.
        
        ìž…ë ¥:
        - site_url: ì²« íŽ˜ì´ì§€ URL
        - keywords: í•„í„°ë§ í‚¤ì›Œë“œ
        - max_articles: ìµœëŒ€ ìˆ˜ì§‘ ê¸€ ìˆ˜
        - max_pages: ìµœëŒ€ íƒìƒ‰ íŽ˜ì´ì§€ ìˆ˜
        - pagination_type: íŽ˜ì´ì§€ ë„˜ê¹€ ë°©ì‹
            * auto: ìžë™ ê°ì§€ (URL íŒ¨í„´ ë¶„ì„)
            * url_param: ?page=N í˜•íƒœ
            * path: /notice/N í˜•íƒœ
            * next_button: "ë‹¤ìŒ" ë§í¬ ì°¾ê¸°
        - page_param: pagination_type=url_paramì¼ ë•Œ ì‚¬ìš©í•  íŒŒë¼ë¯¸í„°ëª…
        - delay_seconds: íŽ˜ì´ì§€ ê°„ ì§€ì—° ì‹œê°„
        
        ì¶œë ¥:
        {
            "site_url": str,
            "pages_crawled": int,
            "found_count": int,
            "articles": [{"title": str, "url": str, "matched_keywords": [str]}, ...]
        }
        """
        if keywords is None:
            keywords = [
                "ë³´ì´ìŠ¤í”¼ì‹±", "ì „í™”ê¸ˆìœµì‚¬ê¸°", "ìŠ¤ë¯¸ì‹±", "í”¼ì‹±",
                "ë©”ì‹ ì €í”¼ì‹±", "ì‚¬ê¸°", "ê¸ˆìœµì‚¬ê¸°", "í…”ë ˆê·¸ëž¨"
            ]
        
        all_articles = []
        current_url = site_url
        pages_crawled = 0
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # íŽ˜ì´ì§€ë„¤ì´ì…˜ íƒ€ìž… ìžë™ ê°ì§€
        if pagination_type == "auto":
            parsed = urlparse(site_url)
            if f'{page_param}=' in parsed.query:
                pagination_type = "url_param"
            elif re.search(r'/\d+/?$', parsed.path):
                pagination_type = "path"
            else:
                pagination_type = "next_button"
        
        for page_num in range(1, max_pages + 1):
            try:
                print(f"ðŸ“„ í¬ë¡¤ë§ ì¤‘: íŽ˜ì´ì§€ {page_num}/{max_pages}")
                
                # íŽ˜ì´ì§€ URL ìƒì„±
                if pagination_type == "url_param":
                    # ?page=N ë°©ì‹
                    parsed = urlparse(site_url)
                    query_params = parse_qs(parsed.query)
                    query_params[page_param] = [str(page_num)]
                    new_query = urlencode(query_params, doseq=True)
                    current_url = urlunparse(parsed._replace(query=new_query))
                    
                elif pagination_type == "path":
                    # /notice/N ë°©ì‹
                    base_url = re.sub(r'/\d+/?$', '', site_url)
                    current_url = f"{base_url}/{page_num}"
                
                # íŽ˜ì´ì§€ ìš”ì²­
                response = requests.get(current_url, headers=headers, timeout=15)
                response.raise_for_status()
                response.encoding = response.apparent_encoding or 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ê¸€ ëª©ë¡ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ìž¬ì‚¬ìš©)
                items = (
                    soup.select('tr') or
                    soup.select('li') or
                    soup.select('article') or
                    soup.select('.board-list tr') or
                    soup.select('.notice-list li')
                )
                
                page_articles = []
                
                for item in items:
                    # ì œëª© ì¶”ì¶œ
                    title_elem = (
                        item.select_one('td.title') or
                        item.select_one('.title') or
                        item.select_one('a') or
                        item.select_one('h3') or
                        item.select_one('h4')
                    )
                    
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    
                    # í‚¤ì›Œë“œ í•„í„°ë§
                    matched_keywords = [kw for kw in keywords if kw in title]
                    if not matched_keywords:
                        continue
                    
                    # ë§í¬ ì¶”ì¶œ
                    link_elem = title_elem if title_elem.name == 'a' else item.select_one('a')
                    
                    if not link_elem:
                        continue
                    
                    href = link_elem.get('href', '')
                    if not href:
                        continue
                    
                    full_url = urljoin(current_url, href)
                    
                    page_articles.append({
                        "title": title,
                        "url": full_url,
                        "matched_keywords": matched_keywords,
                        "page": page_num
                    })
                
                print(f"   â†’ ë°œê²¬: {len(page_articles)}ê°œ")
                all_articles.extend(page_articles)
                pages_crawled += 1
                
                # ìµœëŒ€ ê¸€ ìˆ˜ ë„ë‹¬ ì‹œ ì¤‘ë‹¨
                if len(all_articles) >= max_articles:
                    all_articles = all_articles[:max_articles]
                    break
                
                # ë‹¤ìŒ íŽ˜ì´ì§€ ì°¾ê¸° (next_button ë°©ì‹)
                if pagination_type == "next_button":
                    next_link = (
                        soup.select_one('a.next') or
                        soup.select_one('a[rel="next"]') or
                        soup.select_one('.pagination a:contains("ë‹¤ìŒ")') or
                        soup.select_one('.paging a:contains("ë‹¤ìŒ")')
                    )
                    
                    if not next_link:
                        print("   â†’ ë‹¤ìŒ íŽ˜ì´ì§€ ì—†ìŒ, ì¢…ë£Œ")
                        break
                    
                    next_href = next_link.get('href', '')
                    if not next_href:
                        break
                    
                    current_url = urljoin(current_url, next_href)
                
                # ê¸€ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
                if len(page_articles) == 0:
                    print("   â†’ ê¸€ ì—†ìŒ, ì¢…ë£Œ")
                    break
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€
                time.sleep(delay_seconds)
                
            except Exception as e:
                print(f"   âš ï¸  íŽ˜ì´ì§€ {page_num} ì˜¤ë¥˜: {str(e)}")
                break
        
        return {
            "site_url": site_url,
            "pages_crawled": pages_crawled,
            "found_count": len(all_articles),
            "articles": all_articles
        }


    @tool("crawl_and_extract_batch_multi_page")
    def crawl_and_extract_batch_multi_page(
        site_url: str,
        keywords: List[str] = None,
        max_articles: int = 30,
        max_pages: int = 5,
        pagination_type: str = "auto",
        delay_seconds: float = 2.0,
    ) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ íŽ˜ì´ì§€ í¬ë¡¤ë§ + ë³¸ë¬¸ ì¶”ì¶œì„ í•œë²ˆì— ì²˜ë¦¬í•œë‹¤.
        
        crawl_and_extract_batchì˜ ë‹¤ì¤‘ íŽ˜ì´ì§€ ë²„ì „
        """
        # 1ë‹¨ê³„: ì—¬ëŸ¬ íŽ˜ì´ì§€ì—ì„œ ê¸€ ëª©ë¡ ìˆ˜ì§‘
        crawl_result = crawl_site_with_pagination.invoke({
            "site_url": site_url,
            "keywords": keywords,
            "max_articles": max_articles,
            "max_pages": max_pages,
            "pagination_type": pagination_type,
            "delay_seconds": delay_seconds
        })
        
        if crawl_result.get("found_count", 0) == 0:
            return {
                "site_url": site_url,
                "pages_crawled": 0,
                "crawled_count": 0,
                "extracted_count": 0,
                "articles": [],
                "note": "no_articles_found"
            }
        
        # 2ë‹¨ê³„: ë³¸ë¬¸ ì¶”ì¶œ
        articles_with_content = []
        
        print(f"\nðŸ“ ë³¸ë¬¸ ì¶”ì¶œ ì‹œìž‘: {crawl_result.get('found_count')}ê°œ ê¸€")
        
        for i, article in enumerate(crawl_result.get("articles", []), 1):
            print(f"   {i}/{crawl_result.get('found_count')}: {article['title'][:30]}...")
            
            time.sleep(delay_seconds)
            
            extract_result = extract_article_content.invoke({"article_url": article["url"]})
            
            if extract_result.get("extracted"):
                articles_with_content.append({
                    "title": article["title"],
                    "url": article["url"],
                    "content": extract_result["content"],
                    "matched_keywords": article["matched_keywords"],
                    "page": article.get("page", 1)
                })
        
        print(f"âœ… ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ: {len(articles_with_content)}ê°œ")
        
        return {
            "site_url": site_url,
            "pages_crawled": crawl_result.get("pages_crawled", 0),
            "crawled_count": crawl_result.get("found_count", 0),
            "extracted_count": len(articles_with_content),
            "articles": articles_with_content
        }
    
    # -----------------------------
    # ê¸°ì¡´ í•¨ìˆ˜ë“¤
    # -----------------------------
    @tool("vector_search")
    def vector_search(
        query: str,
        top_k: int = 5,
        min_relevance: float = 0.80,
    ) -> Dict[str, Any]:
        """
        (í˜¸í™˜/ë””ë²„ê·¸ìš©) ChromaDB(VectorDB)ì—ì„œ query ìœ ì‚¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•œë‹¤.
        web-only ê·¸ëž˜í”„ì—ì„œëŠ” í˜¸ì¶œí•˜ì§€ ì•Šì§€ë§Œ, ë„êµ¬ ë“±ë¡ì„ ìœ„í•´ ë‚¨ê²¨ë‘”ë‹¤.

        ë°˜í™˜:
        - route: "HIT" | "MISS"
        - query: str
        - hits: [{content, metadata, score}]
        - scores: [float]
        """
        results = vectordb.similarity_search_with_relevance_scores(query, k=int(top_k))
        hits: List[Dict[str, Any]] = []
        scores: List[float] = []
        for doc, score in results:
            s = float(score)
            scores.append(s)
            hits.append({"content": doc.page_content, "metadata": doc.metadata, "score": s})
        route = "HIT" if any(s >= float(min_relevance) for s in scores) else "MISS"
        return {"route": route, "query": query, "hits": hits, "scores": scores}

    # -----------------------------
    # 2) Tavily search (SNIPPETS ONLY)
    # -----------------------------
    tavily_snippets = TavilySearch(
        max_results=15,
        topic="general",
        include_answer=True,
        include_raw_content=False,  # âœ… ëª¨ë¸ë¡œ ì›ë¬¸ ì•ˆ ì˜¬ë¦¼
        search_depth="basic",
    )

    @tool("web_search_snippets")
    def web_search_snippets(
        query: str,
        topic: str = "general",
        max_results: int = 5,
        time_range: Optional[str] = None,       # day/week/month/year
        include_domains: Optional[List[str]] = None,
        exclude_domains: Optional[List[str]] = None,
        search_depth: Optional[str] = None,     # basic/advanced
    ) -> List[Dict[str, Any]]:
        """
        Tavilyë¡œ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ë˜, ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ í† í° í­ë°œì„ ë§‰ê¸° ìœ„í•´
        raw_content(ë³¸ë¬¸)ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ì•Šê³  'ì§§ì€ ìŠ¤ë‹ˆíŽ«'ë§Œ ë°˜í™˜í•œë‹¤.

        ì‚¬ìš© ëª©ì :
        - ì—ì´ì „íŠ¸ê°€ ì–´ë–¤ URL/ë¬¸ì„œê°€ ìœ ì˜ë¯¸í•œì§€ ë¹ ë¥´ê²Œ íŒë‹¨
        - í›„ì† ë‹¨ê³„ì—ì„œ web_fetch_and_storeë¡œ ì›ë¬¸ì„ ì¶”ì¶œ/ì €ìž¥í•˜ê¸° ìœ„í•œ í›„ë³´ ìˆ˜ì§‘

        ìž…ë ¥:
        - query: ê²€ìƒ‰ ì§ˆì˜
        - topic: "general" | "news" | "finance"
        - max_results: ê²°ê³¼ ê°œìˆ˜(ê¸°ë³¸ 5)
        - time_range: "day" | "week" | "month" | "year"
        - include_domains / exclude_domains: ë„ë©”ì¸ í•„í„°
        - search_depth: "basic" | "advanced"

        ì¶œë ¥(ë¦¬ìŠ¤íŠ¸):
        - ê° í•­ëª©: {title, url, content, score}
        - contentëŠ” ìš”ì•½/ìŠ¤ë‹ˆíŽ« ìˆ˜ì¤€ì˜ ì§§ì€ í…ìŠ¤íŠ¸ë§Œ í¬í•¨
        """
        CACHE_PATH = Path(".cache") / "recent_search_urls.json"
        CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)

        def _load_recent_urls(limit: int = 200) -> list[str]:
            if not CACHE_PATH.exists():
                return []
            try:
                data = json.loads(CACHE_PATH.read_text(encoding="utf-8"))
                if isinstance(data, list):
                    return [str(x) for x in data][-limit:]
            except Exception:
                pass
            return []

        def _save_recent_urls(urls: list[str], limit: int = 200) -> None:
            try:
                CACHE_PATH.write_text(
                    json.dumps(urls[-limit:], ensure_ascii=False),
                    encoding="utf-8",
                )
            except Exception:
                pass

        def _is_hub_url(u: str) -> bool:
            u = u.lower().rstrip("/")
            bad_contains = ["/tag/", "/tags/", "/topic/", "/topics/"]
            bad_exact_end = ["/news", "/crypto/bitcoin/news", "/symbols/btcusd/news"]
            if any(x in u for x in bad_contains):
                return True
            if any(u.endswith(x) for x in bad_exact_end):
                return True
            # ë„ë©”ì¸ ë‹¨ ë©”ì¸
            if u in {"https://coinness.com", "https://coinness.com/"}:
                return True
            return False
        
        queries = [
            query,
            f"{query} ê¸°ê´€ ì‚¬ì¹­",
            f"{query} ê°€ì¡± ì‚¬ì¹­",
            f"{query} ìŠ¤ë¯¸ì‹± ë¬¸ìž ë§í¬",
            f"{query} ì•± ì„¤ì¹˜ ìœ ë„",
        ]

        all_results = []
        seen = set()

        for q in queries:
            args: Dict[str, Any] = {"query": q}

            # invocationì—ì„œ ë°”ê¿€ ìˆ˜ ìžˆëŠ” íŒŒë¼ë¯¸í„°ë§Œ ì„¸íŒ…
            if topic:
                args["topic"] = topic
            if time_range:
                args["time_range"] = time_range
            if include_domains:
                args["include_domains"] = include_domains
            if exclude_domains:
                args["exclude_domains"] = exclude_domains
            if search_depth:
                args["search_depth"] = search_depth

            raw_out = tavily_snippets.invoke(args)
            results = _normalize_tavily_search_output(raw_out)

            for r in results:
                url = (r.get("url") or "").strip()
                if not url or _is_hub_url(url):
                    continue
                if url in seen:
                    continue
                seen.add(url)
                all_results.append(r)

        recent_urls = _load_recent_urls()
        recent_set = set(recent_urls)

        # 1) ìµœê·¼ì— ì“´ URLì€ ìš°ì„  ì œì™¸
        fresh_pool = []
        for r in all_results:
            u = (r.get("url") or "").strip()
            if u and (u not in recent_set):
                fresh_pool.append(r)

        # 2) fresh_poolì´ ë„ˆë¬´ ì ìœ¼ë©´(ìƒˆ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´) ì „ì²´ í’€ë¡œ fallback
        pool = fresh_pool if len(fresh_pool) >= int(max_results) else all_results

        # 3) ì‹¤í–‰ë§ˆë‹¤ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì˜¤ë„ë¡ shuffle í›„ max_results ë§Œí¼ ì„ íƒ
        random.shuffle(pool)
        picked = pool[: int(max_results)]

        cleaned: List[Dict[str, Any]] = []
        picked_urls: list[str] = []
        for r in picked:
            url = (r.get("url") or "").strip()
            cleaned.append(
                {
                    "title": (r.get("title") or "").strip(),
                    "url": url,
                    "content": (r.get("content") or "").strip()[:800],
                    "score": r.get("score"),
                }
            )
            if url:
                picked_urls.append(url)

        # 4) ì´ë²ˆì— ë½‘ì€ URLì„ ìºì‹œì— ëˆ„ì  ì €ìž¥(ìµœê·¼ URL íšŒí”¼ìš©)
        _save_recent_urls(recent_urls + picked_urls)

        return cleaned

    # -----------------------------
    # 3) Search -> Extract -> Store (ê°€ìž¥ ì•ˆì •)
    # -----------------------------
    tavily_for_urls = TavilySearch(
        max_results=5,
        topic="general",
        include_answer=False,
        include_raw_content=False,  # âœ… URL/ìŠ¤ë‹ˆíŽ«ë§Œ
        search_depth="basic",
    )
    tavily_extract = TavilyExtract(extract_depth="advanced", include_images=False)

    @tool("web_fetch_and_store")
    def web_fetch_and_store(
        query: str,
        topic: str = "general",
        max_results: int = 5,
        time_range: Optional[str] = None,
        include_domains: Optional[List[str]] = None,
        exclude_domains: Optional[List[str]] = None,
        search_depth: Optional[str] = None,
        kind: str = "web",
        dedup: bool = True,
    ) -> Dict[str, Any]:
        """
        âœ… ì•ˆì • ë²„ì „:
        ì›¹ ìˆ˜ì§‘/ì €ìž¥ ì „ìš© ë„êµ¬.

        Search -> Extract -> Store 2ë‹¨ê³„ë¡œ ë™ìž‘í•˜ì—¬,
        Tavily ê²€ìƒ‰ ê²°ê³¼ì—ì„œ URLì„ ì–»ì€ ë’¤ TavilyExtractë¡œ ë³¸ë¬¸(content)ì„ ì¶”ì¶œí•˜ê³ ,
        ê·¸ ë³¸ë¬¸ì„ ChromaDBì— ì €ìž¥í•œë‹¤.

        ì¤‘ìš”:
        - ë³¸ë¬¸(raw_content/content)ì€ ëª¨ë¸ì—ê²Œ ë°˜í™˜í•˜ì§€ ì•ŠëŠ”ë‹¤.
        (ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ í† í° ì¦ê°€/ë¹„ìš© ì¦ê°€/429 ìœ„í—˜ì„ ë°©ì§€)
        - ëª¨ë¸ì—ê²ŒëŠ” ì €ìž¥ ê²°ê³¼(ì €ìž¥ ê°œìˆ˜, ìŠ¤í‚µ ê°œìˆ˜, ì‚¬ìš©í•œ sources ë“±)ë§Œ ë°˜í™˜í•œë‹¤.

        ìž…ë ¥:
        - query: ê²€ìƒ‰ ì§ˆì˜
        - topic: "general" | "news" | "finance"
        - max_results: URL í›„ë³´ ê°œìˆ˜
        - time_range: "day" | "week" | "month" | "year"
        - include_domains / exclude_domains: ë„ë©”ì¸ í•„í„°
        - search_depth: "basic" | "advanced"
        - kind: ì €ìž¥ ë©”íƒ€ë°ì´í„° êµ¬ë¶„ê°’(ê¸°ë³¸ "web")
        - dedup: url+content_hash ê¸°ë°˜ ì¤‘ë³µ ì œê±° ì—¬ë¶€

        ì¶œë ¥(dict):
        - stored: ì €ìž¥ëœ ë¬¸ì„œ ìˆ˜
        - skipped: ë³¸ë¬¸ ì—†ìŒ/ì¤‘ë³µ ë“±ìœ¼ë¡œ ìŠ¤í‚µëœ ìˆ˜
        - kind/query: ê¸°ë¡ìš© í•„ë“œ
        - sources: (ìµœëŒ€ max_results) [{title, url}]
        """
        now = datetime.now(timezone.utc).isoformat()

        # (1) URL ìˆ˜ì§‘
        args: Dict[str, Any] = {"query": query}
        if topic:
            args["topic"] = topic
        if time_range:
            args["time_range"] = time_range
        if include_domains:
            args["include_domains"] = include_domains
        if exclude_domains:
            args["exclude_domains"] = exclude_domains
        if search_depth:
            args["search_depth"] = search_depth

        search_out = tavily_for_urls.invoke(args)
        search_results = _normalize_tavily_search_output(search_out)

        urls: List[str] = []
        sources: List[Dict[str, str]] = []
        for r in search_results[:5]:
            u = (r.get("url") or "").strip()
            if u:
                urls.append(u)
                sources.append({"title": (r.get("title") or "").strip(), "url": u})

        if not urls:
            return {"stored": 0, "skipped": 0, "kind": kind, "query": query, "note": "no_urls_from_search"}

        # (2) Extractë¡œ ë³¸ë¬¸ ë½‘ê¸°
        extract_out = tavily_extract.invoke({"urls": urls})
        # TavilyExtractëŠ” ë³´í†µ dict í˜•íƒœë¡œ ì˜¤ë©°, urlsë³„ contentë¥¼ í¬í•¨
        # ë²„ì „ì°¨ë¥¼ ëŒ€ë¹„í•´ ìµœëŒ€í•œ ìœ ì—°í•˜ê²Œ íŒŒì‹±
        extracted_items: List[Tuple[str, str]] = []

        if isinstance(extract_out, dict):
            # ì˜ˆìƒ: {"results": [{"url":..., "content":...}, ...]} ë˜ëŠ” {"url":..., "content":...} ë³€í˜•
            if isinstance(extract_out.get("results"), list):
                for item in extract_out["results"]:
                    if isinstance(item, dict):
                        u = (item.get("url") or "").strip()
                        c = (item.get("content") or "").strip()
                        if u and c:
                            extracted_items.append((u, c))
            else:
                u = (extract_out.get("url") or "").strip()
                c = (extract_out.get("content") or "").strip()
                if u and c:
                    extracted_items.append((u, c))

        # (3) ì €ìž¥ (dedupì€ url+hash)
        docs: List[Document] = []
        seen_keys = set()
        skipped = 0

        for u, content in extracted_items:
            content_hash = _hash_text(content[:20000])
            key = f"{u}::{content_hash}"
            if dedup and key in seen_keys:
                skipped += 1
                continue
            seen_keys.add(key)

            title = ""
            for s in sources:
                if s["url"] == u:
                    title = s.get("title", "")
                    break

            docs.append(
                Document(
                    page_content=content,
                    metadata={
                        "source": u,
                        "title": title,
                        "fetched_at": now,
                        "query": query,
                        "kind": kind,
                        "content_hash": content_hash,
                    },
                )
            )

        if docs:
            vectordb.add_documents(docs)

        stored = len(docs)
        # ì¶”ì¶œ ì‹¤íŒ¨(0ê°œ)ì¼ ë•Œë„ ìµœì†Œ ìŠ¤ë‹ˆíŽ« ì €ìž¥ fallbackì„ í•˜ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸°ì„œ ì¶”ê°€ ê°€ëŠ¥

        return {"stored": stored, "skipped": skipped, "kind": kind, "query": query, "sources": sources[:max_results]}

    # -----------------------------
    # 4) í˜¸í™˜ìš© web_search
    # -----------------------------
    @tool("web_search")
    def web_search(
        query: str,
        topic: str = "general",
        max_results: int = 5,
        time_range: Optional[str] = None,
        include_domains: Optional[List[str]] = None,
        exclude_domains: Optional[List[str]] = None,
        search_depth: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        í˜¸í™˜(Backward compatibility)ìš© ì›¹ ê²€ìƒ‰ ë„êµ¬.

        ê¸°ì¡´ ì½”ë“œ/í”„ë¡¬í”„íŠ¸ê°€ web_search(...)ë¥¼ í˜¸ì¶œí•˜ë”ë¼ë„ ê¹¨ì§€ì§€ ì•Šë„ë¡,
        ë‚´ë¶€ì ìœ¼ë¡œ web_search_snippets(...)ë¥¼ í˜¸ì¶œí•´ ë™ì¼í•œ í˜•ì‹ì˜ 'ì§§ì€ ìŠ¤ë‹ˆíŽ«' ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.

        ì£¼ì˜:
        - ì›ë¬¸ ì¶”ì¶œ/ì €ìž¥ì€ í•˜ì§€ ì•ŠëŠ”ë‹¤.
        - ì €ìž¥ì´ í•„ìš”í•˜ë©´ web_fetch_and_store(...)ë¥¼ ë³„ë„ë¡œ í˜¸ì¶œí•´ì•¼ í•œë‹¤.
        """
        return web_search_snippets(
            query=query,
            topic=topic,
            max_results=max_results,
            time_range=time_range,
            include_domains=include_domains,
            exclude_domains=exclude_domains,
            search_depth=search_depth,
        )
    
    @tool("report_write_and_store")
    def report_write_and_store(
        query_used: str,
        sources: List[Dict[str, str]],
        snippets: List[Dict[str, Any]],
        stored: int = 0,
        skipped: int = 0,
        report_kind: str = "web_report",
    ) -> Dict[str, Any]:
        """
        ì›¹ ê²€ìƒ‰ ê²°ê³¼(ìŠ¤ë‹ˆíŽ«/ë§í¬)ë¥¼ LLMì´ 'ë¦¬í¬íŠ¸'ë¡œ ìš”ì•½/ì •ë¦¬í•˜ê³ ,
        ì›ë³¸ ë§í¬ ëª©ë¡ê³¼ í•¨ê»˜ ChromaDBì— 1ê°œ ë¬¸ì„œë¡œ ì €ìž¥í•œë‹¤.

        ìž…ë ¥:
        - query_used: ì‹¤ì œ ì‚¬ìš©í•œ ê²€ìƒ‰ ì¿¼ë¦¬
        - sources: [{title, url}]
        - snippets: [{title,url,content,score}]
        - stored/skipped: (ì„ íƒ) ì›ë¬¸ ì €ìž¥ ë„êµ¬ ê²°ê³¼ë¥¼ í•¨ê»˜ ê¸°ë¡
        ì¶œë ¥:
        - stored_report: 1(ì„±ê³µ) ë˜ëŠ” 0
        - report_id: content_hash
        """

        # LLM (ì§§ê²Œ, êµ¬ì¡°í™”)
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, timeout=30)

        # ìŠ¤ë‹ˆíŽ« í…ìŠ¤íŠ¸ë¥¼ ë„ˆë¬´ ê¸¸ê²Œ ë³´ë‚´ì§€ ì•Šë„ë¡ ì»·
        lines = []
        for i, s in enumerate(snippets[:8], 1):
            title = (s.get("title") or "").strip()
            url = (s.get("url") or "").strip()
            content = (s.get("content") or "").strip()
            content = content[:800]  # ê° ìŠ¤ë‹ˆíŽ« ê¸¸ì´ ì œí•œ
            lines.append(f"{i}. {title}\n- url: {url}\n- snippet: {content}")

        prompt = f"""
        ë„ˆëŠ” ë³´ì´ìŠ¤í”¼ì‹± ìµœì‹  ìˆ˜ë²•ì„ 'ìœ í˜•ë³„ ì§€ì‹ë² ì´ìŠ¤'ë¡œ ì •ë¦¬í•˜ëŠ” ë¶„ì„ê°€ë‹¤.
        ì£¼ì œëŠ” ë°˜ë“œì‹œ: ë³´ì´ìŠ¤í”¼ì‹± ìµœì‹  ìˆ˜ë²•

        ì•„ëž˜ ì›¹ ê²€ìƒ‰ ìŠ¤ë‹ˆíŽ«ê³¼ ë§í¬ë¥¼ ê·¼ê±°ë¡œ, ìµœì‹  ìˆ˜ë²•ì„ 'ìœ í˜•(type)' ë‹¨ìœ„ë¡œ ë¶„ë¥˜í•´ì„œ
        ë°˜ë“œì‹œ ì•„ëž˜ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ ì¶œë ¥í•˜ë¼. (ë§ˆí¬ë‹¤ìš´/ì„¤ëª… ê¸ˆì§€, JSONë§Œ)

        [JSON ìŠ¤í‚¤ë§ˆ]
        {{
        "topic": "ë³´ì´ìŠ¤í”¼ì‹± ìµœì‹  ìˆ˜ë²•",
        "as_of": "{datetime.now(timezone.utc).date().isoformat()}",
        "types": [
            {{
            "type": "ìœ í˜•ëª…(ì˜ˆ: ê¸°ê´€ ì‚¬ì¹­, ê°€ì¡±/ì§€ì¸ ì‚¬ì¹­, ëŒ€ì¶œ ì‚¬ê¸°, íƒë°°/ë¬¸ìž ë§í¬, ëª¸ìº /í˜‘ë°•, ì•Œë°”/êµ¬ì¸, ì¤‘ê³ ê±°ëž˜, íˆ¬ìž/ì½”ì¸ ë“±)",
            "keywords": ["ì£¼ìš” í‚¤ì›Œë“œ1","í‚¤ì›Œë“œ2",],
            "scenario": ["1) ë‹¨ê³„ë³„ ì‹œë‚˜ë¦¬ì˜¤", "2) ...", "3) ...", ...],
            "red_flags": ["ì˜ì‹¬ ì‹ í˜¸ 1", "ì˜ì‹¬ ì‹ í˜¸ 2",],
            "recommended_actions": ["ëŒ€ì‘ 1", "ëŒ€ì‘ 2",]
            }}
        ],
        "sources": [
            {{"title":"...","url":"..."}}
        ]
        }}

        ê·œì¹™:
        - typesëŠ” ê°€ëŠ¥í•œ í•œ 5~12ê°œ ì‚¬ì´ë¡œ ë½‘ì•„ë¼.
        - scenarioëŠ” ë°˜ë“œì‹œ 5~7 ë‹¨ê³„ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì¨ë¼.
        - sourcesëŠ” ì œê³µëœ ë§í¬ë§Œ ì‚¬ìš©í•˜ë¼.
        - ìŠ¤ë‹ˆíŽ« ê·¼ê±°ê°€ ì•½í•˜ë©´ typeì€ ë„£ë˜ scenario/keywordsë¥¼ ë³´ìˆ˜ì ìœ¼ë¡œ ìž‘ì„±í•˜ë¼.

        [ê²€ìƒ‰ ìŠ¤ë‹ˆíŽ«]
        {chr(10).join(lines)}
        """.strip()

        report_md = llm.invoke(prompt).content
        report_json_str = llm.invoke(prompt).content.strip()

        sources_trim = sources[:10]
        # Sourcesë¥¼ í•­ìƒ ë¶™ì—¬ì„œ ì €ìž¥ (LLMì´ ë¹ ëœ¨ë ¤ë„ ë³´ìž¥)
        if sources_trim:
            src_lines = ["\n\n## Sources"]
            for s in sources_trim:
                title = (s.get("title") or "").strip()
                url = (s.get("url") or "").strip()
                if url:
                    src_lines.append(f"- [{title or url}]({url})")
            report_md = report_md.rstrip() + "\n" + "\n".join(src_lines) + "\n"

        now = datetime.now(timezone.utc).isoformat()
        report_hash = _hash_text(report_json_str)

        
        # Chromaì— "ë¦¬í¬íŠ¸ 1ê°œ ë¬¸ì„œ" ì €ìž¥
        doc = Document(
            page_content=report_json_str,   # âœ… JSONì„ ê·¸ëŒ€ë¡œ ì €ìž¥
            metadata={
                "kind": "voicephishing_types_v1",
                "query": "ë³´ì´ìŠ¤í”¼ì‹± ìµœì‹  ìˆ˜ë²•",
                "created_at": now,
                "sources_count": int(len(sources_trim)),
                "snippets_count": int(len(snippets or [])),
                "content_hash": report_hash,
            },
        )
        vectordb.add_documents([doc])

        return {"stored_report": 1, "report_id": report_hash, "kind": "voicephishing_types_v1"}
    
    @tool("store_snippets_only")
    def store_snippets_only(
        query_used: str,
        snippets: List[Dict[str, Any]],
        kind: str = "voicephishing_snippet_v1",
    ) -> Dict[str, Any]:
        """
        LLM ì—†ì´ ì›¹ ê²€ìƒ‰ ìŠ¤ë‹ˆíŽ«(title/url/content)ë§Œ ChromaDBì— ì €ìž¥í•œë‹¤.
        - ê¸°ì‚¬ 1ê°œ = ë¬¸ì„œ 1ê°œ
        - metadataëŠ” Chroma ì œì•½(ì›ì‹œíƒ€ìž…)ë§Œ ì‚¬ìš©í•œë‹¤.
        """
        now = datetime.now(timezone.utc).isoformat()

        stored = 0
        skipped = 0
        docs: List[Document] = []

        for s in (snippets or []):
            title = (s.get("title") or "").strip()
            url = (s.get("url") or "").strip()
            content = (s.get("content") or "").strip()
            snippet_id = _hash_text(url)  # âœ… URL ê¸°ë°˜ ê³ ìœ  ID

            if not url:
                skipped += 1
                continue

            # ë„ˆë¬´ ê¸¸ë©´ ìžë¦„ (ì €ìž¥ìš©)
            content = content[:600]

            payload = {
                "topic": "ë³´ì´ìŠ¤í”¼ì‹± ìµœì‹  ìˆ˜ë²•",
                "query_used": query_used,
                "article": {"title": title, "url": url},
                "snippet": content,
                "created_at": now,
                "snippet_id": snippet_id,
            }
            page_content = json.dumps(payload, ensure_ascii=False)

            content_hash = _hash_text(url + "|" + content)

            docs.append(
                Document(
                    page_content=page_content,
                    metadata={
                        "kind": kind,
                        "query": query_used,
                        "title": title[:200],
                        "url": url,
                        "created_at": now,
                        "content_hash": content_hash,
                        # âœ… ì¶”ê°€
                        "snippet_id": snippet_id,
                        "processed": False,
                        "used_in_report_id": "", 
                    },
                )
            )
            stored += 1

        if docs:
            vectordb.add_documents(docs)

        return {"stored": stored, "skipped": skipped, "kind": kind}
    

    # =========================
    # 1) LOAD: ìˆ˜ì§‘ëœ ìŠ¤ë‹ˆíŽ« ë¡œë“œ
    # =========================
    @tool("load_collected_snippets")
    def load_collected_snippets(
        limit: int = 5,
        kind: str = "voicephishing_snippet_v1",
        only_unprocessed: bool = True,
    ) -> Dict[str, Any]:
        """
        ChromaDBì— ì €ìž¥ëœ ìˆ˜ì§‘(snippet) ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¨ë‹¤.
        ìš”ì•½/ë¦¬í¬íŠ¸ ë‹¨ê³„ì—ì„œ LLM ìž…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.

        ë°˜í™˜:
        {
        "count": N,
        "items": [
            {
            "doc_id": "...",        # Chroma ë‚´ë¶€ ë¬¸ì„œ ID
            "snippet_id": "...",    # URL ê¸°ë°˜ ê³ ìœ  ID(ì—†ìœ¼ë©´ None)
            "title": "...",
            "url": "...",
            "created_at": "...",
            "payload_json": "..."   # store_snippets_onlyê°€ ì €ìž¥í•œ page_content(JSON ë¬¸ìžì—´)
            }, ...
        ]
        }
        """
        # langchain_chromaëŠ” ë‚´ë¶€ì— _collection(Chroma Collection)ì„ ë“¤ê³  ìžˆìŒ
        col = vectordb._collection  # privateì§€ë§Œ ì‹¤ë¬´ì—ì„œ ë§Žì´ ì”€

        if only_unprocessed:
            where = {
                "$and": [
                    {"kind": {"$eq": kind}},
                    {"processed": {"$eq": False}},
                ]
            }
        else:
            where = {"kind": {"$eq": kind}}

        data = col.get(where=where, limit=int(limit), include=["documents", "metadatas"])

        items: List[Dict[str, Any]] = []
        for doc_id, content, meta in zip(data.get("ids", []), data.get("documents", []), data.get("metadatas", [])):
            items.append(
                {
                    "doc_id": doc_id,  # chroma ë‚´ë¶€ id (ìžˆìœ¼ë©´ ìœ ìš©)
                    "snippet_id": meta.get("snippet_id"),
                    "title": meta.get("title"),
                    "url": meta.get("url"),
                    "created_at": meta.get("created_at"),
                    "payload_json": content,  # store_snippets_onlyê°€ ë„£ì€ JSON ë¬¸ìžì—´
                }
            )

        return {"count": len(items), "items": items}
    
    # ==========================================
    # 2) WRITE+STORE: ìŠ¤ë‹ˆíŽ«ë“¤ -> ë¦¬í¬íŠ¸ ì €ìž¥
    # ==========================================
    @tool("write_report_from_snippets_and_store")
    def write_report_from_snippets_and_store(
        query_used: str,
        snippet_items: List[Dict[str, Any]],
        report_kind: str = "voicephishing_report_v1",
    ) -> Dict[str, Any]:
        """
        ìˆ˜ì§‘ëœ snippet ì—¬ëŸ¬ ê°œë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì´ ìš”ì•½ ë¦¬í¬íŠ¸ë¥¼ ìž‘ì„±í•˜ê³  ChromaDBì— ì €ìž¥í•œë‹¤.
        ë¦¬í¬íŠ¸ëŠ” ìˆ˜ì§‘ ë¬¸ì„œë“¤ê³¼ ì—°ê²°ë  ìˆ˜ ìžˆë„ë¡ source_snippet_ids_jsonì„ metadataì— ì €ìž¥í•œë‹¤.

        ë°˜í™˜:
        {"stored_report": 1, "report_id": "...", "source_count": N}
        """
        # LLM 1íšŒë§Œ
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, timeout=20, max_retries=1)

        # LLM ìž…ë ¥ìš© ìš”ì•½: payload_jsonì—ì„œ snippetë§Œ ë½‘ì•„ ì§§ê²Œ êµ¬ì„±
        normalized: List[Dict[str, Any]] = []
        source_doc_ids: List[str] = []
        source_snippet_ids: List[str] = []

        for it in snippet_items:
            doc_id = (it.get("doc_id") or "").strip()
            source_doc_ids.append(doc_id)

            sid = it.get("snippet_id") or ""
            if not sid:
                # snippet_idê°€ ì—†ë˜ êµ¬ ë°ì´í„° ëŒ€ë¹„: urlë¡œ ë§Œë“¤ì–´ì¤Œ
                # (ê°€ëŠ¥í•˜ë©´ ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œ snippet_idë¥¼ í•­ìƒ ë„£ë„ë¡ ê¶Œìž¥)
                try:
                    payload_tmp = json.loads(it.get("payload_json") or "{}")
                    url_tmp = ((payload_tmp.get("article") or {}).get("url") or it.get("url") or "").strip()
                except Exception:
                    url_tmp = (it.get("url") or "").strip()
                sid = _hash_text(url_tmp) if url_tmp else _hash_text(doc_id or json.dumps(it, ensure_ascii=False))
            source_snippet_ids.append(sid)

            try:
                payload = json.loads(it.get("payload_json") or "{}")
            except Exception:
                payload = {}

            title = ((payload.get("article") or {}).get("title") or it.get("title") or "").strip()
            url = ((payload.get("article") or {}).get("url") or it.get("url") or "").strip()
            snippet = (payload.get("snippet") or "").strip()

            normalized.append(
                {
                    "snippet_id": sid,
                    "title": str(title)[:160],
                    "url": str(url),
                    "snippet": str(snippet)[:700],
                }
            )

        if not normalized:
            return {"stored_report": 0, "report_id": None, "source_count": 0, "reason": "no_snippets"}

        now = datetime.now(timezone.utc).isoformat()

        prompt = f"""
    ë„ˆëŠ” ë³´ì´ìŠ¤í”¼ì‹± ìµœì‹  ìˆ˜ë²•ì„ ë¶„ì„í•´ 'ë¦¬í¬íŠ¸'ë¡œ ì •ë¦¬í•˜ëŠ” ë¶„ì„ê°€ë‹¤.
    ìž…ë ¥ì€ ì—¬ëŸ¬ ê°œì˜ ë‰´ìŠ¤ ìŠ¤ë‹ˆíŽ«ì´ë©°, ê° ìŠ¤ë‹ˆíŽ«ì—ëŠ” snippet_idê°€ ìžˆë‹¤.

    ì¶œë ¥ í˜•ì‹(ë°˜ë“œì‹œ ì§€ì¼œë¼):
    - ìœ í˜• ë‹¨ìœ„ë¡œ ì„¹ì…˜ì„ ë‚˜ëˆ„ì–´ ìž‘ì„±:
    ìœ í˜•: ...
    ì£¼ìš” í‚¤ì›Œë“œ: ... (ì—¬ëŸ¬ ê°œ)
    ì‹œë‚˜ë¦¬ì˜¤:
        1. ...
        2. ...
        3. ...
    ê·¼ê±° snippet_id: ["...","..."]  (ì´ ìœ í˜•ì„ ë’·ë°›ì¹¨í•˜ëŠ” snippet_idë“¤ì„ ë°˜ë“œì‹œ í¬í•¨)

    - ë§ˆì§€ë§‰ì—ëŠ” ì•„ëž˜ ì˜ˆì‹œì²˜ëŸ¼ "ì¢…í•© ë¶„ì„ ë¬¸ë‹¨" 1ê°œë¥¼ ìž‘ì„±í•˜ë¼:
    (ì˜ˆì‹œ í†¤) "í”¼í•´ìžëŠ” ê¶Œìœ„ì™€ ì „ë¬¸ì„±ì„ ì¸ì§€í•˜ì—¬ ..."

    ê·œì¹™:
    - ìŠ¤ë‹ˆíŽ«ì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ ì •ë³´ë§Œ ì‚¬ìš©í•˜ê³  ê³¼ìž¥/ì°½ìž‘ ê¸ˆì§€
    - "ê·¼ê±° snippet_id"ëŠ” ë°˜ë“œì‹œ JSON ë°°ì—´ í˜•íƒœë¡œ í‘œê¸°
    - ì „ì²´ ì¶œë ¥ì€ í•œêµ­ì–´ í…ìŠ¤íŠ¸(ë§ˆí¬ë‹¤ìš´ í—ˆìš©), ì½”ë“œíŽœìŠ¤ ê¸ˆì§€

    [ìž…ë ¥ ìŠ¤ë‹ˆíŽ«ë“¤]
    {json.dumps(normalized, ensure_ascii=False)}
    """.strip()

        report_text = llm.invoke(prompt).content.strip()

        now = datetime.now(timezone.utc).isoformat()
        report_id = _hash_text(report_text + "|" + now)

        doc = Document(
            page_content=report_text,
            metadata={
                "kind": report_kind,
                "query": query_used,
                "created_at": now,
                "report_id": report_id,
                # Chroma metadata ì œì•½ ë•Œë¬¸ì— JSON ë¬¸ìžì—´ë¡œ ì €ìž¥
                "source_snippet_ids_json": json.dumps(source_snippet_ids, ensure_ascii=False),
                "source_doc_ids_json": json.dumps(source_doc_ids, ensure_ascii=False),
                "source_count": int(len(source_snippet_ids)),
            },
        )
        vectordb.add_documents([doc])

        return {"stored_report": 1, "report_id": report_id, "source_count": len(source_snippet_ids)}
    
    @tool("mark_snippets_processed")
    def mark_snippets_processed(
        doc_ids: List[str],
        report_id: str,
        kind: str = "voicephishing_snippet_v1",
    ) -> Dict[str, Any]:
        """
        ìˆ˜ì§‘(snippet) ë¬¸ì„œë¥¼ processed=Trueë¡œ ì—…ë°ì´íŠ¸í•˜ê³ ,
        ì–´ë–¤ report_idì—ì„œ ì‚¬ìš©í–ˆëŠ”ì§€ report_idë„ ê¸°ë¡í•œë‹¤.

        doc_idsëŠ” load_collected_snippetsê°€ ëŒë ¤ì¤€ items[*].doc_id ë¦¬ìŠ¤íŠ¸ë¥¼ ë„£ëŠ”ë‹¤.
        """

        col = vectordb._collection

        # Chroma updateëŠ” ids ê¸°ì¤€ìœ¼ë¡œ ê°€ëŠ¥
        # metadataëŠ” ì „ì²´ë¥¼ ë®ì–´ì“¸ ìˆ˜ ìžˆìœ¼ë‹ˆ, ê¸°ì¡´ metadataë¥¼ ë¨¼ì € ê°€ì ¸ì™€ ë³‘í•©í•˜ëŠ” ë°©ì‹ì´ ì•ˆì „
        data = col.get(ids=doc_ids, include=["metadatas"])
        old_metas = data.get("metadatas", []) or []

        new_metas: List[Dict[str, Any]] = []
        for meta in old_metas:
            meta = dict(meta or {})
            meta["kind"] = kind
            meta["processed"] = True
            meta["used_in_report_id"] = str(report_id or "")
            meta["processed_at"] = datetime.now(timezone.utc).isoformat()
            new_metas.append(meta)

        col.update(ids=doc_ids, metadatas=new_metas)
        return {"updated": len(doc_ids), "report_id": report_id}

    return [vector_search, 
            web_search_snippets, 
            web_fetch_and_store, 
            web_search, 
            report_write_and_store, 
            store_snippets_only, 
            load_collected_snippets, 
            write_report_from_snippets_and_store, 
            mark_snippets_processed,
            generate_targeted_guidance,
            store_guidance_to_db,
            search_existing_guidance,
            # í¬ë¡¤ë§ ë„êµ¬ ì¶”ê°€
            crawl_site_for_phishing_cases,
            extract_article_content,
            crawl_and_extract_batch,
            crawl_site_with_pagination,
            crawl_and_extract_batch_multi_page,
            generate_guidance_from_crawled_articles,
            store_crawled_guidance
            ]
