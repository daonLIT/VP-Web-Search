# scripts/check_guidance_db.py
import json
from langchain_openai import OpenAIEmbeddings
from app.tools.store import get_chroma

embeddings = OpenAIEmbeddings()
vectordb = get_chroma(embeddings)

# Chroma collection ì ‘ê·¼
col = vectordb._collection

# === 1. í†µí•© ê²€ìƒ‰ìœ¼ë¡œ ìƒì„±ëœ ì§€ì¹¨ í™•ì¸ ===
print("="*60)
print("ğŸ” í†µí•© ê²€ìƒ‰ìœ¼ë¡œ ìƒì„±ëœ ì§€ì¹¨ (voicephishing_guidance_v1)")
print("="*60)

where = {"kind": {"$eq": "voicephishing_guidance_v1"}}
data = col.get(where=where, limit=20, include=["documents", "metadatas"])

guidance_count = len(data.get("ids", []))
print(f"\nì´ {guidance_count}ê°œ ì§€ì¹¨ ì €ì¥ë¨\n")

for i, (doc_id, content, meta) in enumerate(zip(
    data.get("ids", []), 
    data.get("documents", []), 
    data.get("metadatas", [])
), 1):
    print(f"[{i}] ID: {doc_id[:30]}...")
    print(f"    ìœ í˜•: {meta.get('phishing_type')}")
    print(f"    ìƒì„±ì¼: {meta.get('created_at')}")
    print(f"    Guidance ID: {meta.get('guidance_id', 'N/A')[:20]}...")
    print(f"    ì¶œì²˜: {meta.get('source_system', 'N/A')}")
    
    # JSON íŒŒì‹±í•´ì„œ ì£¼ìš” ì •ë³´ í‘œì‹œ
    try:
        guidance_data = json.loads(content)
        print(f"    í‚¤ì›Œë“œ: {', '.join(guidance_data.get('keywords', [])[:3])}...")
        print(f"    ì‹œë‚˜ë¦¬ì˜¤ ë‹¨ê³„: {len(guidance_data.get('scenario', []))}ê°œ")
        print(f"    ì¶œì²˜ ìˆ˜: {len(guidance_data.get('sources', []))}ê°œ")
    except:
        print(f"    ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content[:80]}...")
    
    print("-" * 60)

# === 2. í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„±ëœ ì§€ì¹¨ í™•ì¸ ===
print("\n" + "="*60)
print("ğŸ•·ï¸  í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„±ëœ ì§€ì¹¨ (voicephishing_guidance_crawled_v1)")
print("="*60)

where_crawled = {"kind": {"$eq": "voicephishing_guidance_crawled_v1"}}
data_crawled = col.get(where=where_crawled, limit=20, include=["documents", "metadatas"])

crawled_count = len(data_crawled.get("ids", []))
print(f"\nì´ {crawled_count}ê°œ í¬ë¡¤ë§ ì§€ì¹¨ ì €ì¥ë¨\n")

for i, (doc_id, content, meta) in enumerate(zip(
    data_crawled.get("ids", []), 
    data_crawled.get("documents", []), 
    data_crawled.get("metadatas", [])
), 1):
    print(f"[{i}] ID: {doc_id[:30]}...")
    print(f"    ìœ í˜•: {meta.get('phishing_type')}")
    print(f"    ì¶œì²˜ ì‚¬ì´íŠ¸: {meta.get('source_site', 'N/A')[:50]}...")
    print(f"    ìƒì„±ì¼: {meta.get('created_at')}")
    
    try:
        guidance_data = json.loads(content)
        print(f"    í‚¤ì›Œë“œ: {', '.join(guidance_data.get('keywords', [])[:3])}...")
    except:
        pass
    
    print("-" * 60)

# === 3. ì „ì²´ í†µê³„ ===
print("\n" + "="*60)
print("ğŸ“Š ì „ì²´ í†µê³„")
print("="*60)

all_kinds = col.get(limit=1000, include=["metadatas"])
kind_counts = {}

for meta in all_kinds.get("metadatas", []):
    kind = meta.get("kind", "unknown")
    kind_counts[kind] = kind_counts.get(kind, 0) + 1

print("\nKindë³„ ë¬¸ì„œ ìˆ˜:")
for kind, count in sorted(kind_counts.items(), key=lambda x: x[1], reverse=True):
    print(f"  - {kind}: {count}ê°œ")

print(f"\nì´ ë¬¸ì„œ ìˆ˜: {sum(kind_counts.values())}ê°œ")